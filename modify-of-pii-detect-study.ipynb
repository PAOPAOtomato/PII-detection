{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":159367535,"sourceType":"kernelVersion"}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### This notebook is modified from <a href=\"https://www.kaggle.com/code/pjmathematician/pii-eda-presidio-baseline\">PII EDA Presidio Baseline</a> and <a href=\"https://www.kaggle.com/code/yunsuxiaozi/pii-detect-study-notebook\">PII detect study notebook</a>. ","metadata":{}},{"cell_type":"markdown","source":"## Modifications \n\n#### I add my own address_recognizer and email_recognizer, URL_recognizer, and add a black list to filter potential public urls and date checker to filter noisy phone numbers. I also added Chinese note for my modifications.","metadata":{}},{"cell_type":"markdown","source":"### Install presidio","metadata":{}},{"cell_type":"code","source":"#安装python库 presidio_analyzer 不从python库里下载,而是从给定的链接处下载,更新到最新版本,并减少输出信息.\n!pip install -U -q presidio_analyzer --no-index --find-links=file:///kaggle/input/presidio-wheels/presidio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-20T07:40:57.486577Z","iopub.execute_input":"2024-01-20T07:40:57.487624Z","iopub.status.idle":"2024-01-20T07:41:13.824483Z","shell.execute_reply.started":"2024-01-20T07:40:57.48757Z","shell.execute_reply":"2024-01-20T07:41:13.823011Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Import  necessary libraries","metadata":{}},{"cell_type":"code","source":"import json#用于处理json格式数据的库\nimport pandas as pd#导入csv文件的库\n#Presidio 是一个开源的文本分析库,用于提取文本的敏感信息.\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\nfrom tqdm import tqdm\nfrom typing import List\nimport pprint\nimport re\n\nfrom presidio_analyzer import AnalyzerEngine, PatternRecognizer, EntityRecognizer, Pattern, RecognizerResult\nfrom presidio_analyzer.recognizer_registry import RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NlpArtifacts\nfrom presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n\nfrom presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\nfrom presidio_analyzer.predefined_recognizers import PhoneRecognizer\nfrom dateutil import parser","metadata":{"execution":{"iopub.status.busy":"2024-01-20T07:41:13.826484Z","iopub.execute_input":"2024-01-20T07:41:13.826839Z","iopub.status.idle":"2024-01-20T07:41:18.852087Z","shell.execute_reply.started":"2024-01-20T07:41:13.826808Z","shell.execute_reply":"2024-01-20T07:41:18.850875Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Import dataset","metadata":{}},{"cell_type":"code","source":"train_df=json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"))\nprint(f\"len(train_df):{len(train_df)},train_df[0].keys():{train_df[0].keys()}\")\nprint(\"-\"*50)\nlabels=set()\nfor i in range(len(train_df)):\n    labels.update(train_df[i]['labels'])\nprint(f\"labels:{labels}\")\ntest_df = json.load(open('/kaggle/input/pii-detection-removal-from-educational-data/test.json'))","metadata":{"execution":{"iopub.status.busy":"2024-01-20T07:41:18.853153Z","iopub.execute_input":"2024-01-20T07:41:18.853694Z","iopub.status.idle":"2024-01-20T07:41:21.255201Z","shell.execute_reply.started":"2024-01-20T07:41:18.853647Z","shell.execute_reply":"2024-01-20T07:41:21.254008Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### create Analyzer","metadata":{}},{"cell_type":"code","source":"# analyzer = AnalyzerEngine()#创建文本分析器\nconfiguration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n}\n\n# Create NLP engine based on configuration\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine = provider.create_engine()\n\n# create address recognizer  创建地址分析器\naddress_regex = r'\\b\\d+\\s+\\w+(\\s+\\w+)*\\s+((st(\\.)?)|(ave(\\.)?)|(rd(\\.)?)|(blvd(\\.)?)|(ln(\\.)?)|(ct(\\.)?)|(dr(\\.)?))\\b'\naddress_pattern = Pattern(name=\"address\", regex=address_regex, score=0.5)\naddress_recognizer = PatternRecognizer(supported_entity=\"ADDRESS_CUSTOM\", patterns = [address_pattern], context=[\"st\", \"Apt\", \"email\"])\n\n# create address recognizer  创建邮箱分析器 \nemail_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\nemail_pattern = Pattern(name=\"email address\", regex=email_regex, score=0.5)\nemail_recognizer = PatternRecognizer(supported_entity=\"EMAIL_CUSTOM\", patterns = [email_pattern])\n\n# create url recognizer  创建URL分析器 \nurl_regex = r'https?://\\S+|www\\.\\S+'\nurl_pattern = Pattern(name=\"url\", regex=url_regex, score=0.5)\nurl_recognizer = PatternRecognizer(supported_entity=\"URL_CUSTOM\", patterns = [url_pattern])\n\n# create phone recognizer  创建电话分析器 \nphone_recognizer = PhoneRecognizer(context=['phone', 'number', 'telephone', 'cell', 'cellphone', 'mobile', 'call', 'ph', 'tel', 'mobile', 'Email'])\n\n\nregistry = RecognizerRegistry()\nregistry.load_predefined_recognizers()\nregistry.add_recognizer(address_recognizer)\nregistry.add_recognizer(email_recognizer)\nregistry.add_recognizer(url_recognizer)\nregistry.add_recognizer(phone_recognizer)\n\n\n# Pass the created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine, \n    supported_languages=[\"en\"],\n    registry=registry,\n    context_aware_enhancer=\n        LemmaContextAwareEnhancer(context_similarity_factor=0.6, min_score_with_context_similarity=0.4)\n)\n\n# remove date info in phone number recognizer  移除日期类型的电话号码\ndef is_valid_date(text):\n    try:\n        # Attempt to parse the text as a date\n        parsed_date = parser.parse(text)\n        return True\n    except:\n        return False","metadata":{"execution":{"iopub.status.busy":"2024-01-20T07:41:21.257563Z","iopub.execute_input":"2024-01-20T07:41:21.257949Z","iopub.status.idle":"2024-01-20T07:41:25.944228Z","shell.execute_reply.started":"2024-01-20T07:41:21.257917Z","shell.execute_reply":"2024-01-20T07:41:25.943032Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Function","metadata":{}},{"cell_type":"code","source":"#对文本进行分词成下标,也就是每个词的起始位置和终止位置\ndef tokens2index(row):#传入一个json解析的数据\n    tokens  = row['tokens']#分词的数据['apple','bool','cat',……]\n    start_ind = []\n    end_ind = []\n    prev_ind = 0\n    for tok in tokens:#取出一个词\n        #比如现在的位置是30,从30开始往后找index为5,那么起始位置就是35\n        start = prev_ind + row['full_text'][prev_ind:].index(tok)\n        end = start+len(tok)#起始位置+词的长度=终点位置\n        #储存这个词的起点和终点位置\n        start_ind.append(start)\n        end_ind.append(end)\n        prev_ind = end\n    return start_ind, end_ind#返回的是分词后每个词的起始位置和终点位置\n\n#二分查找,找到arr[index]=target\ndef find_or_next_larger(arr, target):#arr:分词后每个词的start,target:一个实体的start\n    left, right = 0, len(arr) - 1#arr的最左边和最右边\n\n    while left <= right:\n        mid = (left + right) // 2\n\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return left\ndef count_trailing_whitespaces(word):\n    #单词的长度-单词去掉尾部空格后的长度=单词尾部的长度\n    return len(word) - len(word.rstrip())","metadata":{"execution":{"iopub.status.busy":"2024-01-20T07:41:25.945739Z","iopub.execute_input":"2024-01-20T07:41:25.946007Z","iopub.status.idle":"2024-01-20T07:41:25.95386Z","shell.execute_reply.started":"2024-01-20T07:41:25.945983Z","shell.execute_reply":"2024-01-20T07:41:25.953069Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prediction","metadata":{}},{"cell_type":"code","source":"# Add URL black list 创建URL黑名单\nblack_list = [\"wikipedia\", \"coursera\", \".pdf\", \".PDF\", \"article\", \".png\", \".gov\", \".work\", \".ai\", \".firm\", \".arts\", \".store\", \".rec\", \".biz\", \".travel\" ]\nwhite_list = ['phone', 'number', 'telephone', 'cell', 'cellphone', 'mobile', 'call', 'ph', 'tel', 'mobile', 'Email', 'email', 'Apt', 'apt']","metadata":{"execution":{"iopub.status.busy":"2024-01-20T07:41:25.954935Z","iopub.execute_input":"2024-01-20T07:41:25.955218Z","iopub.status.idle":"2024-01-20T07:41:26.157296Z","shell.execute_reply.started":"2024-01-20T07:41:25.955192Z","shell.execute_reply":"2024-01-20T07:41:26.155642Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_ = test_df #test_df #train_df\nPHONE_NUM, NAME_STUDENT, URL_PERSONAL, EMAIL, STREET_ADDRESS, ID_NUM, USERNAME = [],[],[],[],[],[], []\n\npreds = []\n#查找每个词分词后的起始位置和终点位置\nfor i in tqdm(range(len(df_)), desc=\"Processing tokens2index\"):\n    start, end = tokens2index(df_[i])\n    #将每个词分词后的起始位置和终点位置加入json文件里.\n    df_[i]['start'] = start\n    df_[i]['end'] = end\n    \nfor i, d in tqdm(enumerate(df_), total=len(df_), desc=\"Analyzing entities\"):#取出d=df_[i]\n    #传入的文本是full_text,对英文文本进行分析,需要识别的是电话号码,人,url和email这几种类型.\n    #results:[type: PERSON, start: 22, end: 37, score: 0.85]\n    results = analyzer.analyze(text=d['full_text'],\n                           entities=[\"PHONE_NUMBER\", \n                                     \"PERSON\", \n                                     \"URL_CUSTOM\", # \"IP_ADDRESS\", #\"URL\",\n                                     \"EMAIL_CUSTOM\", \"EMAIL_ADDRESS\",\n                                     \"ADDRESS_CUSTOM\",\n                                     \"US_SSN\",  \"US_PASSPORT\", \"US_BANK_NUMBER\", #\"US_ITIN\",\n                                     \"USERNAME\"],\n                           language='en',\n#                            score_threshold=0.005,\n                            )\n    pre_preds = []\n    for r in results:#遍历找到过的每个实体,r:[type: PERSON, start: 22, end: 37, score: 0.85]\n        #就是第s个词就是某个实体的开始\n        s = find_or_next_larger(d['start'], r.start)#d['start'][s]=r.start\n        end = r.end#实体终点\n        word = d['full_text'][r.start:r.end]#文本里找单词\n        end = end - count_trailing_whitespaces(word)#end减去尾部的空格就是单词自身尾部的下标\n        temp_preds = [s]#实体单词的集合从第s个单词开始,然后连续几个单词?\n        try:\n            #实体可能不是一个单词,分词的下一个单词如果还没有到达实体的尾部,就把下一个单词加上\n            while d['end'][s+1] <= end:\n                temp_preds.append(s+1)\n                s +=1\n        except:\n            pass\n        \n        #找出来的实体是什么,我们就给它打对应的标签\n        tmp = False\n        \n        if r.entity_type == 'USERNAME':\n            label =  'USERNAME'\n            USERNAME.append(d['full_text'][r.start:r.end])\n            \n        if r.entity_type == 'PHONE_NUMBER':\n            #检查是不是日期类型\n            if is_valid_date(word):\n                continue\n            for w in white_list:\n                if w in d['full_text'][max(r.start-100, 0):min(r.end+100, len(d['full_text']))]:\n                    tmp = False\n                    break\n                else:\n                    tmp = True \n                    \n            label =  'PHONE_NUM'\n            PHONE_NUM.append(d['full_text'][r.start:r.end])\n            \n        if r.entity_type == 'PERSON':\n            label =  'NAME_STUDENT'\n            NAME_STUDENT.append(d['full_text'][r.start:r.end])\n            \n        if r.entity_type == 'ADDRESS_CUSTOM':\n            label = 'STREET_ADDRESS'\n            STREET_ADDRESS.append(d['full_text'][r.start:r.end])\n            \n        if r.entity_type == 'US_SSN' or r.entity_type == 'US_ITIN' or r.entity_type == 'US_PASSPORT' or r.entity_type == 'US_BANK_NUMBER':\n            label = 'ID_NUM'\n            ID_NUM.append(d['full_text'][r.start:r.end])\n            \n        if r.entity_type == 'EMAIL_ADDRESS' or r.entity_type == 'EMAIL_CUSTOM':\n            label = 'EMAIL'\n            EMAIL.append(d['full_text'][r.start:r.end])\n            \n        if r.entity_type == 'URL_CUSTOM':# or r.entity_type == 'IP_ADDRESS' or \"http\" in word:\n#             for w in white_list:\n#                 if w in d['full_text'][max(r.start-150, 0):min(r.end+150, len(d['full_text']))]:\n#                     tmp = False\n#                     break\n#                 else:\n#                     tmp = True \n            #去除掉黑名单里的标签\n            for w in black_list:\n                if w in word:\n                    tmp = True\n                    break\n            \n            label = 'URL_PERSONAL'\n            URL_PERSONAL.append(d['full_text'][r.start:r.end])\n            \n        if tmp:\n            continue\n        \n            \n        #取出实体中的一个分词的下标\n        for p in temp_preds:\n            if len(pre_preds) > 0:#第2次及以后经过这里.\n                \"\"\"\n                新开始一个r的时候,pre_preds[-1]['rlabel']还是上一个实体的r.entity_type\n                此时也许会不等于这个实体的r.entity_type,换句话说,第一个等号就是还在同一个实体里.\n                p - pre_preds[-1]['token']==1就是连续的意思\n                \"\"\"\n                if pre_preds[-1]['rlabel'] == r.entity_type and (p - pre_preds[-1]['token']==1):\n                    label_f = \"I-\"+label#实体的中间位置\n                else:\n                    label_f = \"B-\"+label#否则就是下一个实体的开始\n            else:#第一个label是起始位置,故标记为‘B-’\n                label_f = \"B-\"+label\n            #保存document,从第p个单词开始,标签为label_f\n            pre_preds.append(({\n                    \"document\":d['document'],\n                    \"token\":p,\n                    \"label\":label_f,\n                    \"rlabel\":r.entity_type#实体的类型\n                }))\n    preds.extend(pre_preds)#遍历完这个数据之后,将所有找到的实体做汇总","metadata":{"execution":{"iopub.status.busy":"2024-01-20T07:41:26.158587Z","iopub.execute_input":"2024-01-20T07:41:26.158911Z","iopub.status.idle":"2024-01-20T07:52:54.113034Z","shell.execute_reply.started":"2024-01-20T07:41:26.158885Z","shell.execute_reply":"2024-01-20T07:52:54.111889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"#得到预测结果后,最后一行r.entity_type不要,reset_index\nsubmission = pd.DataFrame(preds).iloc[:,:-1].reset_index()\n#index变成row_id,剩下3列就是submission的列名\nsubmission.columns = ['row_id','document', 'token', 'label']\n#保存csv文件\nsubmission.to_csv('submission.csv', index = False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-20T07:52:54.114315Z","iopub.execute_input":"2024-01-20T07:52:54.114603Z","iopub.status.idle":"2024-01-20T07:52:54.19626Z","shell.execute_reply.started":"2024-01-20T07:52:54.114574Z","shell.execute_reply":"2024-01-20T07:52:54.1952Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}