{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7466758,"sourceType":"datasetVersion","datasetId":4332496},{"sourceId":159367535,"sourceType":"kernelVersion"}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### This notebook is modified from <a href=\"https://www.kaggle.com/code/leonshangguan/modify-of-pii-detect-study\">Modify of PII Detect Study</a>, <a href=\"https://www.kaggle.com/code/pjmathematician/pii-eda-presidio-baseline\">PII EDA Presidio Baseline</a> and <a href=\"https://www.kaggle.com/code/yunsuxiaozi/pii-detect-study-notebook\">PII detect study notebook</a>. ","metadata":{}},{"cell_type":"code","source":"# Modifications \n\nFirstly, big thanks to the users who provided the notebooks above. This notebook is merely adding some utility code to make a solid baseline that other users may iterate on, but all the heavy lifting was done by the above notebooks.\n\nI encapsulated the analyzer in a class, and added code to run the analyzer on (potentially) both the training and test set. I also added validation code, so that we can analyze the performance of the analyzer on the training set.\n\nI also added a global configuration for ease of testing, which allows the user to switch between training and inference mode. Additionally, I incorporated the external data that [https://www.kaggle.com/alejopaullier](@moth) kindly provided in his discussion post.\n\nFor now, the business logic roughly stays the same as the Modify of PII Detect Study notebook that I used beforehand.\n\n## Resources\n\n* My EDA notebook: https://www.kaggle.com/code/mcpenguin/eda-pii-detection-removal/notebook\n* Customizing the presidio analyzer: https://microsoft.github.io/presidio/samples/python/customizing_presidio_analyzer/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Version History\n\n- v16: Original baseline\n- v17: Changed score thresholds for patterns from 0.5 -> 0.8\n- v20: fixed bug in evaluation code\n- v21: changed url regex to not require \"www\"\n- v23: reverted v21, added @amed's metric code\n- v24: added custom ID pattern","metadata":{}},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"class CONFIG:\n    \"\"\"\n    > General Options\n    \"\"\"\n    # global seed\n    seed = 42\n    # number of samples to use for testing purposes\n    # if None, we use the whole training dataset\n    samples_testing = None\n    # flag to indicate whether to use the external training dataset\n    # or just to use the original data\n    use_external_train_data = True\n    # whether to run the algorithm on the training set and do subsequent validation\n    # with 6.8k rows, this takes almost 50 minutes to run\n    run_on_train_data = True\n    \n    \"\"\"\n    > Analyzer Options\n    \"\"\"\n    # score threshold for patterns\n    id_pattern_score = 0.8\n    address_pattern_score = 0.8\n    email_pattern_score = 0.8\n    url_pattern_score = 0.8","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:07:27.515496Z","iopub.execute_input":"2024-02-14T21:07:27.516147Z","iopub.status.idle":"2024-02-14T21:07:27.562229Z","shell.execute_reply.started":"2024-02-14T21:07:27.516082Z","shell.execute_reply":"2024-02-14T21:07:27.561047Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"markdown","source":"### Install presidio","metadata":{}},{"cell_type":"code","source":"#安装python库 presidio_analyzer 不从python库里下载,而是从给定的链接处下载,更新到最新版本,并减少输出信息.\n!pip install -U -q presidio_analyzer --no-index --find-links=file:///kaggle/input/presidio-wheels/presidio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-14T21:07:27.564941Z","iopub.execute_input":"2024-02-14T21:07:27.565473Z","iopub.status.idle":"2024-02-14T21:07:49.365138Z","shell.execute_reply.started":"2024-02-14T21:07:27.565430Z","shell.execute_reply":"2024-02-14T21:07:49.363663Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Import  necessary libraries","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\n\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\nfrom tqdm import tqdm\nfrom typing import List\nimport random\nimport pprint\nimport re\nimport gc\nfrom ast import literal_eval\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import fbeta_score, classification_report, confusion_matrix\n\nfrom presidio_analyzer import AnalyzerEngine, PatternRecognizer, EntityRecognizer, Pattern, RecognizerResult\nfrom presidio_analyzer.recognizer_registry import RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NlpArtifacts\nfrom presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n\nfrom dateutil import parser","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:07:49.366784Z","iopub.execute_input":"2024-02-14T21:07:49.367184Z","iopub.status.idle":"2024-02-14T21:07:56.177449Z","shell.execute_reply.started":"2024-02-14T21:07:49.367149Z","shell.execute_reply":"2024-02-14T21:07:56.176508Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:07:56.182155Z","iopub.execute_input":"2024-02-14T21:07:56.183814Z","iopub.status.idle":"2024-02-14T21:07:56.189884Z","shell.execute_reply.started":"2024-02-14T21:07:56.183766Z","shell.execute_reply":"2024-02-14T21:07:56.189116Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Define Metric (F1 Beta Score)","metadata":{}},{"cell_type":"code","source":"# Big thanks to \n# https://www.kaggle.com/code/amedprof/pii-evaluation-metric?scriptVersionId=160040455\ndef pii_fbeta_score(pred_df, gt_df,beta=5):\n    \"\"\"\n    Parameters:\n    - pred_df (DataFrame): DataFrame containing predicted PII labels.\n    - gt_df (DataFrame): DataFrame containing ground truth PII labels.\n    - beta (float): The beta parameter for the F-beta score, controlling the trade-off between precision and recall.\n\n    Returns:\n    - float: Micro F-beta score.\n    \"\"\"   \n    df = pred_df.merge(gt_df,how='outer',on=['document',\"token\"],suffixes=('_pred','_gt'))\n\n    df['cm'] = \"\"\n\n    df.loc[df.label_gt.isna(),'cm'] = \"FP\"\n\n\n    df.loc[df.label_pred.isna(),'cm'] = \"FN\"\n    df.loc[(df.label_gt.notna()) & (df.label_gt!=df.label_pred),'cm'] = \"FN\"\n\n    df.loc[(df.label_pred.notna()) & (df.label_gt.notna()) & (df.label_gt==df.label_pred),'cm'] = \"TP\"\n    \n    FP = (df['cm']==\"FP\").sum()\n    FN = (df['cm']==\"FN\").sum()\n    TP = (df['cm']==\"TP\").sum()\n\n    s_micro = (1+(beta**2))*TP/(((1+(beta**2))*TP) + ((beta**2)*FN) + FP)\n\n    return s_micro","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:07:56.192872Z","iopub.execute_input":"2024-02-14T21:07:56.193441Z","iopub.status.idle":"2024-02-14T21:07:56.326886Z","shell.execute_reply.started":"2024-02-14T21:07:56.193402Z","shell.execute_reply":"2024-02-14T21:07:56.325768Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Import Datasets","metadata":{}},{"cell_type":"markdown","source":"## Import Original Data","metadata":{}},{"cell_type":"code","source":"train_df = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"))\nprint(f\"len(train_df):{len(train_df)}, train_df[0].keys(): {list(train_df[0].keys())}\")\nprint(\"-\"*50)\n\ntest_df = json.load(open('/kaggle/input/pii-detection-removal-from-educational-data/test.json'))","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:07:56.328305Z","iopub.execute_input":"2024-02-14T21:07:56.328664Z","iopub.status.idle":"2024-02-14T21:07:59.341266Z","shell.execute_reply.started":"2024-02-14T21:07:56.328631Z","shell.execute_reply":"2024-02-14T21:07:59.340145Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"len(train_df):6807, train_df[0].keys(): ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels']\n--------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load External Data (if needed)","metadata":{}},{"cell_type":"code","source":"if CONFIG.use_external_train_data:\n    # Convert the \"stringified lists\" in the columns to proper Python lists\n    df_train_external = pd.read_csv('/kaggle/input/pii-external-dataset/pii_dataset.csv', converters={\n        'tokens': literal_eval, \n        'labels': literal_eval, \n        'trailing_whitespace': literal_eval\n    })\n    df_train_external.rename(columns={'text': 'full_text'}, inplace=True)\n    # convert to format similar to how we load in the original data\n    df_train_external = df_train_external.to_dict('records')\n    train_df.extend(df_train_external)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:07:59.342560Z","iopub.execute_input":"2024-02-14T21:07:59.343097Z","iopub.status.idle":"2024-02-14T21:08:09.109617Z","shell.execute_reply.started":"2024-02-14T21:07:59.343064Z","shell.execute_reply":"2024-02-14T21:08:09.108497Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Sample Data (if needed)","metadata":{}},{"cell_type":"code","source":"if CONFIG.samples_testing != None:\n    train_df = random.sample(train_df, CONFIG.samples_testing)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:08:09.110941Z","iopub.execute_input":"2024-02-14T21:08:09.111260Z","iopub.status.idle":"2024-02-14T21:08:09.116638Z","shell.execute_reply.started":"2024-02-14T21:08:09.111231Z","shell.execute_reply":"2024-02-14T21:08:09.115535Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(f\"train_df length:\", len(train_df))\n\nlabels = set()\nlabel_counts = {}\nfor i in range(len(train_df)):\n    labels.update(train_df[i]['labels'])\n    for label in train_df[i]['labels']:\n        if label in label_counts:\n            label_counts[label] += 1\n        else:\n            label_counts[label] = 1\n            \nprint(f\"labels: {labels}\")\nprint('-'*25)\nprint(f\"label_counts: {label_counts}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:08:09.117932Z","iopub.execute_input":"2024-02-14T21:08:09.118782Z","iopub.status.idle":"2024-02-14T21:08:11.142065Z","shell.execute_reply.started":"2024-02-14T21:08:09.118750Z","shell.execute_reply":"2024-02-14T21:08:11.140890Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"train_df length: 11241\nlabels: {'B-STREET_ADDRESS', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-URL_PERSONAL', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-USERNAME', 'I-ID_NUM', 'B-EMAIL', 'B-ID_NUM', 'I-STREET_ADDRESS', 'B-URL_PERSONAL', 'O'}\n-------------------------\nlabel_counts: {'O': 6323308, 'B-NAME_STUDENT': 12469, 'I-NAME_STUDENT': 6763, 'B-URL_PERSONAL': 730, 'B-EMAIL': 3833, 'B-ID_NUM': 78, 'I-URL_PERSONAL': 1, 'B-USERNAME': 724, 'B-PHONE_NUM': 2425, 'I-PHONE_NUM': 3404, 'B-STREET_ADDRESS': 3545, 'I-STREET_ADDRESS': 8597, 'I-ID_NUM': 1}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Helper Methods","metadata":{}},{"cell_type":"code","source":"def is_valid_date(text):\n    try:\n        # Attempt to parse the text as a date\n        parsed_date = parser.parse(text)\n        return True\n    except:\n        return False\n    \ndef tokens2index(row):\n    tokens  = row['tokens']\n    start_ind = []\n    end_ind = []\n    prev_ind = 0\n    for tok in tokens:\n        start = prev_ind + row['full_text'][prev_ind:].index(tok)\n        end = start+len(tok)\n        start_ind.append(start)\n        end_ind.append(end)\n        prev_ind = end\n    return start_ind, end_ind\n\n# binary search\ndef find_or_next_larger(arr, target):\n    left, right = 0, len(arr) - 1\n\n    while left <= right:\n        mid = (left + right) // 2\n\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return left\n\ndef count_trailing_whitespaces(word):\n    return len(word) - len(word.rstrip())","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:08:11.144032Z","iopub.execute_input":"2024-02-14T21:08:11.144887Z","iopub.status.idle":"2024-02-14T21:08:11.156696Z","shell.execute_reply.started":"2024-02-14T21:08:11.144843Z","shell.execute_reply":"2024-02-14T21:08:11.155713Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Create Analyzer\n\nFor ease of code, we encapsulate the analyzer code in a class.","metadata":{}},{"cell_type":"code","source":"class MyAnalyzer:\n    \n    def __init__(self):\n        ## Initialize the analyzer\n        configuration = {\n            \"nlp_engine_name\": \"spacy\",\n            \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n        }\n        \n        # Create NLP engine based on configuration\n        provider = NlpEngineProvider(nlp_configuration=configuration)\n        nlp_engine = provider.create_engine()\n        \n        # create ID recognizer\n        id_regex = r'([A-Za-z]{2}[.?]:)?\\d{12,12}'\n        id_pattern = Pattern(name=\"id\", regex=id_regex, score = CONFIG.id_pattern_score)\n        id_recognizer = PatternRecognizer(supported_entity=\"ID_CUSTOM\", patterns = [id_pattern])\n\n        # create address recognizer\n        address_regex = r'\\b\\d+\\s+\\w+(\\s+\\w+)*\\s+((st(\\.)?)|(ave(\\.)?)|(rd(\\.)?)|(blvd(\\.)?)|(ln(\\.)?)|(ct(\\.)?)|(dr(\\.)?))\\b'\n        address_pattern = Pattern(name=\"address\", regex=address_regex, score = CONFIG.address_pattern_score)\n        address_recognizer = PatternRecognizer(supported_entity=\"ADDRESS_CUSTOM\", patterns = [address_pattern])\n\n        # create email recognizer\n        email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n        email_pattern = Pattern(name=\"email address\", regex=email_regex, score = CONFIG.email_pattern_score)\n        email_recognizer = PatternRecognizer(supported_entity=\"EMAIL_CUSTOM\", patterns = [email_pattern])\n\n        # create url recognizer \n        url_regex = r'https?://\\S+|www\\.\\S+'\n#         url_regex = r'https?:\\/\\/[a-zA-Z1-9.\\?\\=\\&]+'\n        url_pattern = Pattern(name=\"url\", regex=url_regex, score=CONFIG.url_pattern_score)\n        url_recognizer = PatternRecognizer(supported_entity=\"URL_CUSTOM\", patterns = [url_pattern])\n\n        registry = RecognizerRegistry()\n        registry.load_predefined_recognizers()\n        registry.add_recognizer(id_recognizer)\n        registry.add_recognizer(address_recognizer)\n        registry.add_recognizer(email_recognizer)\n        registry.add_recognizer(url_recognizer)\n\n        # Pass the created NLP engine and supported_languages to the AnalyzerEngine\n        self.analyzer = AnalyzerEngine(\n            nlp_engine=nlp_engine, \n            supported_languages=[\"en\"],\n            registry=registry\n        )\n        \n        ## Initialize the black list\n        self.black_list = [\"wikipedia\", \"coursera\", \".pdf\", \".PDF\", \"article\", \n                           \".png\", \".gov\", \".work\", \".ai\", \".firm\", \".arts\", \n                           \".store\", \".rec\", \".biz\", \".travel\" ]\n        \n     \n    def predict_tokens(self, df_: list) -> pd.DataFrame:\n        \"\"\"Predict the tokens that have PII in the dataframe.\"\"\"\n        \n        # list of all predictions for each label\n        PHONE_NUM, NAME_STUDENT, URL_PERSONAL, EMAIL, STREET_ADDRESS, ID_NUM, USERNAME = [],[],[],[],[],[], []\n\n        preds = []\n        \n        # Find the starting and ending positions of each word after segmentation\n        for i in tqdm(range(len(df_)), desc=\"Processing tokens2index\"):\n            start, end = tokens2index(df_[i])\n            df_[i]['start'] = start\n            df_[i]['end'] = end\n\n        for i, d in tqdm(enumerate(df_), total=len(df_), desc=\"Analyzing entities\"):\n            #results:[type: PERSON, start: 22, end: 37, score: 0.85]\n            results = self.analyzer.analyze(text=d['full_text'],\n                                   entities=[\n                                             #\"PHONE_NUMBER\", \n                                             \"PERSON\", \n                                             \"URL_CUSTOM\", #\"IP_ADDRESS\", #\"URL\",\n                                             \"EMAIL_ADDRESS\", \"EMAIL_CUSTOM\", \n                                             \"ADDRESS_CUSTOM\",\n                                             \"US_SSN\", \"US_ITIN\", \"US_PASSPORT\", \"US_BANK_NUMBER\",\n                                             \"USERNAME\"],\n                                   language='en',\n        #                            score_threshold=0.2,\n                                    )\n            pre_preds = []\n            \n            # Traverse each entity found, \n            # r: [type: PERSON, start: 22, end: 37, score: 0.85]\n            for r in results:\n                # That is, the sth word is the beginning of an entity\n                s = find_or_next_larger(d['start'], r.start)# d['start'][s] = r.start\n                \n                end = r.end # entity end point\n                # find words in text\n                word = d['full_text'][r.start:r.end]\n                end = end - count_trailing_whitespaces(word)\n                temp_preds = [s]\n                \n                try:\n                    while d['end'][s+1] <= end:\n                        temp_preds.append(s+1)\n                        s +=1\n                except:\n                    pass\n\n                tmp = False\n\n                if r.entity_type == 'USERNAME':\n                    label = 'USERNAME'\n                    USERNAME.append(d['full_text'][r.start:r.end])\n\n        #         if r.entity_type == 'PHONE_NUMBER':\n        #             if is_valid_date(word):\n        #                 continue\n        #             label =  'PHONE_NUM'\n        #             PHONE_NUM.append(d['full_text'][r.start:r.end])\n\n                if r.entity_type == 'PERSON':\n                    label = 'NAME_STUDENT'\n                    NAME_STUDENT.append(d['full_text'][r.start:r.end])\n\n                if r.entity_type == 'ADDRESS_CUSTOM':\n                    label = 'STREET_ADDRESS'\n                    STREET_ADDRESS.append(d['full_text'][r.start:r.end])\n\n                if r.entity_type == 'ID_CUSTOM' or \\\n                    r.entity_type == 'US_SSN' or r.entity_type == 'US_ITIN' or r.entity_type == 'US_PASSPORT' or r.entity_type == 'US_BANK_NUMBER':\n                    \n                    label = 'ID_NUM'\n                    ID_NUM.append(d['full_text'][r.start:r.end])\n\n                if r.entity_type == 'EMAIL_ADDRESS' or r.entity_type == 'EMAIL_CUSTOM':\n                    label = 'EMAIL'\n                    EMAIL.append(d['full_text'][r.start:r.end])\n\n                if r.entity_type == 'URL_CUSTOM':# or r.entity_type == 'IP_ADDRESS' or \"http\" in word:\n                    for w in self.black_list:\n                        if w in word:\n                            tmp = True\n                            break\n\n                    label = 'URL_PERSONAL'\n                    URL_PERSONAL.append(d['full_text'][r.start:r.end])\n\n                if tmp:\n                    continue\n\n                for p in temp_preds:\n                    if len(pre_preds) > 0:\n                        \"\"\"\n                        When starting a new r, pre_preds[-1]['rlabel'] \n                        is still the r.entity_type of the previous entity\n                        At this time, it may not be equal to the \n                        r.entity_type of this entity. \n                        \n                        In other words, \n                        the first equal sign is still in the same entity.\n                        \"\"\"\n                        if pre_preds[-1]['rlabel'] == r.entity_type and (p - pre_preds[-1]['token']==1):\n                            label_f = \"I-\"+label\n                        else:\n                            label_f = \"B-\"+label\n                    else:\n                        label_f = \"B-\"+label\n                    \n                    # pre_preds contains the output that we want\n                    pre_preds.append(({\n                        \"document\": d['document'],\n                        \"token\": p,\n                        \"label\": label_f,\n                        \"rlabel\": r.entity_type,\n                    }))\n                    \n            # After traversing this data, summarize all found entities\n            # and extend the preds for this document into the aggregate preds\n            preds.extend(pre_preds)\n            \n        preds_df = pd.DataFrame(preds).iloc[:,:-1].reset_index()\n        return preds_df","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:08:11.158352Z","iopub.execute_input":"2024-02-14T21:08:11.159110Z","iopub.status.idle":"2024-02-14T21:08:11.191759Z","shell.execute_reply.started":"2024-02-14T21:08:11.159063Z","shell.execute_reply":"2024-02-14T21:08:11.190838Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Predict Train Set","metadata":{}},{"cell_type":"code","source":"analyzer = MyAnalyzer()","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:08:11.193282Z","iopub.execute_input":"2024-02-14T21:08:11.194009Z","iopub.status.idle":"2024-02-14T21:08:17.329663Z","shell.execute_reply.started":"2024-02-14T21:08:11.193950Z","shell.execute_reply":"2024-02-14T21:08:17.328432Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"if CONFIG.run_on_train_data:\n    train_preds = analyzer.predict_tokens(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:08:17.333004Z","iopub.execute_input":"2024-02-14T21:08:17.333552Z","iopub.status.idle":"2024-02-14T21:30:34.977360Z","shell.execute_reply.started":"2024-02-14T21:08:17.333514Z","shell.execute_reply":"2024-02-14T21:30:34.976326Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Processing tokens2index: 100%|██████████| 11241/11241 [00:09<00:00, 1203.31it/s]\nAnalyzing entities: 100%|██████████| 11241/11241 [22:08<00:00,  8.46it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluate Performance on Training Set","metadata":{}},{"cell_type":"markdown","source":"## Generate Corresponding DataFrame for \"True\" Answers","metadata":{}},{"cell_type":"code","source":"if CONFIG.run_on_train_data:\n    train_act_records = []\n    count = 0\n    for entry in train_df:\n        for idx, (token, label) in enumerate(zip(entry[\"tokens\"], entry[\"labels\"])):\n            if label != 'O':\n                train_act_records.append({\n                    'row_id': count,\n                    'document': entry[\"document\"],\n                    'token': idx,\n                    'label': label,\n                })\n                count += 1\n\n    train_act = pd.DataFrame.from_records(train_act_records)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:30:34.981457Z","iopub.execute_input":"2024-02-14T21:30:34.981811Z","iopub.status.idle":"2024-02-14T21:30:36.808264Z","shell.execute_reply.started":"2024-02-14T21:30:34.981780Z","shell.execute_reply":"2024-02-14T21:30:36.807164Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Check that we haven't missed out any true values when making the \"train_act_records\"\nif CONFIG.run_on_train_data:\n    assert len(train_act) == sum(label_counts.values()) - label_counts['O'], \\\n        'mismatch between number of true labels in label_counts and train_act_records'","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:30:36.810032Z","iopub.execute_input":"2024-02-14T21:30:36.810404Z","iopub.status.idle":"2024-02-14T21:30:36.816099Z","shell.execute_reply.started":"2024-02-14T21:30:36.810371Z","shell.execute_reply":"2024-02-14T21:30:36.814818Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def get_pred_act_lists_for_dfs(preds: pd.DataFrame, act: pd.DataFrame):\n    document_idx_list = [(ex[\"document\"], len(ex[\"tokens\"])) for ex in train_df]\n    \n    preds_list = []\n    act_list = []\n    \n    for document, len_tokens in tqdm(document_idx_list, total=len(document_idx_list)):\n        preds_doc = preds[preds[\"document\"] == document].sort_values(by=\"token\")\n        act_doc = act[act[\"document\"] == document].sort_values(by=\"token\")\n        \n        # We do a \"merge\" (like in mergesort) to combine the results from the preds and \n        # actual values\n        preds_idx = 0\n        act_idx = 0\n        preds_list_sub = []\n        act_list_sub = []\n        for i in range(len_tokens):\n            preds_head, act_head = None, None\n            if preds_idx < len(preds_doc):\n                preds_head = preds_doc.iloc[preds_idx]\n            if act_idx < len(act_doc):\n                act_head = act_doc.iloc[act_idx]\n                \n            if act_head is not None and act_head[\"token\"] == i:\n                act_list_sub.append(act_head[\"label\"])\n                act_idx += 1\n            else:\n                act_list_sub.append('O')\n                \n            if preds_head is not None and preds_head[\"token\"] == i:\n                preds_list_sub.append(preds_head[\"label\"])\n                preds_idx += 1\n            else:\n                preds_list_sub.append('O')\n        \n        preds_list.extend(preds_list_sub)\n        act_list.extend(act_list_sub)\n            \n    return preds_list, act_list\n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:30:36.818070Z","iopub.execute_input":"2024-02-14T21:30:36.818495Z","iopub.status.idle":"2024-02-14T21:30:36.831531Z","shell.execute_reply.started":"2024-02-14T21:30:36.818453Z","shell.execute_reply":"2024-02-14T21:30:36.830509Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"if CONFIG.run_on_train_data:\n    print(\"PII micro F-beta score:\", pii_fbeta_score(train_preds, train_act, beta = 5))","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:30:36.832493Z","iopub.execute_input":"2024-02-14T21:30:36.832826Z","iopub.status.idle":"2024-02-14T21:30:36.998867Z","shell.execute_reply.started":"2024-02-14T21:30:36.832796Z","shell.execute_reply":"2024-02-14T21:30:36.997749Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"PII micro F-beta score: 0.48751463129145534\n","output_type":"stream"}]},{"cell_type":"code","source":"if CONFIG.run_on_train_data:\n    train_preds_list, train_act_list = get_pred_act_lists_for_dfs(train_preds, train_act)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T21:30:37.000437Z","iopub.execute_input":"2024-02-14T21:30:37.001156Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 71%|███████   | 8008/11241 [04:58<03:26, 15.66it/s]","output_type":"stream"}]},{"cell_type":"code","source":"# make sure that we processed the actual train list properly\nif CONFIG.run_on_train_data:\n    assert dict(Counter(train_act_list)) == label_counts, 'mismatch between label counts in label_counts and train_act_list'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.run_on_train_data:\n    print(\"-\"*25)\n    print(\"Counter for predicted labels:\")\n    print(\"-\"*25)\n    pprint.pprint(dict(Counter(train_preds_list)))\n    print()\n    \n    print(\"-\"*25)\n    print(\"Counter for actual labels:\")\n    print(\"-\"*25)\n    pprint.pprint(dict(Counter(train_act_list)))\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification Report on Training Set","metadata":{}},{"cell_type":"code","source":"if CONFIG.run_on_train_data:\n    print(classification_report(train_preds_list, train_act_list, digits=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## F Beta Score on Training Set","metadata":{}},{"cell_type":"code","source":"# if CONFIG.run_on_train_data:\n#     print(\"Micro F1 Beta Score:\", score(train_preds_list, train_act_list))\n#     print(\"Macro F1 Beta Score:\", macro_score(train_preds_list, train_act_list))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.run_on_train_data:\n    del train_preds_list, train_act_list, train_preds, train_act\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict Test Set","metadata":{}},{"cell_type":"code","source":"test_preds = analyzer.predict_tokens(test_df)\ntest_preds.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame(test_preds)\nsubmission.columns = ['row_id','document', 'token', 'label']\nsubmission.to_csv('submission.csv', index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
