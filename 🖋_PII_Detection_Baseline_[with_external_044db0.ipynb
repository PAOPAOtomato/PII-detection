{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 66653,
          "databundleVersionId": 7500999,
          "sourceType": "competition"
        },
        {
          "sourceId": 7466758,
          "sourceType": "datasetVersion",
          "datasetId": 4332496
        },
        {
          "sourceId": 159367535,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30635,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "ðŸ–‹ PII Detection - Baseline [with external  044db0",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'pii-detection-removal-from-educational-data:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F66653%2F7500999%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240408%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240408T150849Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D457b98efd6a9947da951d572d3f84f3589f5693931715b0d8848192a91b83d918e8d822924f1b8400454fb5e0c94a537f96fb191d614214d64ffc36000fdd3d25fd24e1c01491ca2e4e6620a577e0378b588c119ba2b5ab4ea7ea994c9b15362b608b764fb95bd923bfbef0dfebfe3ed3d65152051b13e9902c50954d0ff41fdd6166678315c3f87c79b5da7f8377c3977a9d78fec10d9d3b2162e986f9a4ebf4c9ab7e8e46f49657115d87b7a651e2d44265eb54ba1fa93f0682bf81aef65f8535615050740022ffae93f5931772203d9b1f1482d36c77b3a976c4416981cc53104849cdea2ba4bb3e19962bca8a8ec6e1cd8dace58f64261e17c1adca2f05d,pii-external-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4332496%2F7466758%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240408%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240408T150849Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dc8c4f9e98ea8b186d27166f7f1a4802f483b975987222bbeaac89aac399c2227e21549a74b4712742f541fe9bbae3d2a24bfc3b937a6447ec550a68159e8ab47d297af144a6726a16bf548fd8af1bb45792f5433bd4bfdeb000b67a03f31d0220cf1718d8bc4efbcef8094f499fc520e0146d68633c272082ffdf17a3890aa0674129668c5cec0241e310abc28317a4fae7698976b905139f628478390fdda19694618a369c2bbe2fcd37b9b14904d58aae46701f7300e90df4751d8050f58d6619ebcccfbaafc9472bf11cff852cd847fca5112af1ea6eb58a5b2aa40a4b0144205c1487a28477f99ad8fdb3844ff3a561f7ca9ea0812f1a0aeaa3335049647'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "EKTOlgZOQW29"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "KdPkFnyoQW2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CONFIG:\n",
        "    \"\"\"\n",
        "    > General Options\n",
        "    \"\"\"\n",
        "    # global seed\n",
        "    seed = 42\n",
        "    # number of samples to use for testing purposes\n",
        "    # if None, we use the whole training dataset\n",
        "    samples_testing = None\n",
        "    # flag to indicate whether to use the external training dataset\n",
        "    # or just to use the original data\n",
        "    use_external_train_data = True\n",
        "    # whether to run the algorithm on the training set and do subsequent validation\n",
        "    # with 6.8k rows, this takes almost 50 minutes to run\n",
        "    run_on_train_data = True\n",
        "\n",
        "    \"\"\"\n",
        "    > Analyzer Options\n",
        "    \"\"\"\n",
        "    # score threshold for patterns\n",
        "    id_pattern_score = 0.8\n",
        "    address_pattern_score = 0.8\n",
        "    email_pattern_score = 0.8\n",
        "    url_pattern_score = 0.8"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:07:27.515496Z",
          "iopub.execute_input": "2024-02-14T21:07:27.516147Z",
          "iopub.status.idle": "2024-02-14T21:07:27.562229Z",
          "shell.execute_reply.started": "2024-02-14T21:07:27.516082Z",
          "shell.execute_reply": "2024-02-14T21:07:27.561047Z"
        },
        "trusted": true,
        "id": "CDS7grFtQW3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "kdVsjMa0QW3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install presidio"
      ],
      "metadata": {
        "id": "piq0POH9QW3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -U -q presidio_analyzer --no-index --find-links=file:///kaggle/input/presidio-wheels/presidio"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-02-14T21:07:27.564941Z",
          "iopub.execute_input": "2024-02-14T21:07:27.565473Z",
          "iopub.status.idle": "2024-02-14T21:07:49.365138Z",
          "shell.execute_reply.started": "2024-02-14T21:07:27.56543Z",
          "shell.execute_reply": "2024-02-14T21:07:49.363663Z"
        },
        "trusted": true,
        "id": "L9e9VANXQW3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import  necessary libraries"
      ],
      "metadata": {
        "id": "W_9LIDCoQW3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "import random\n",
        "import pprint\n",
        "import re\n",
        "import gc\n",
        "from ast import literal_eval\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import fbeta_score, classification_report, confusion_matrix\n",
        "\n",
        "from presidio_analyzer import AnalyzerEngine, PatternRecognizer, EntityRecognizer, Pattern, RecognizerResult\n",
        "from presidio_analyzer.recognizer_registry import RecognizerRegistry\n",
        "from presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NlpArtifacts\n",
        "from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n",
        "\n",
        "from dateutil import parser"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:07:49.366784Z",
          "iopub.execute_input": "2024-02-14T21:07:49.367184Z",
          "iopub.status.idle": "2024-02-14T21:07:56.177449Z",
          "shell.execute_reply.started": "2024-02-14T21:07:49.367149Z",
          "shell.execute_reply": "2024-02-14T21:07:56.176508Z"
        },
        "trusted": true,
        "id": "AJObUIEyQW3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:07:56.182155Z",
          "iopub.execute_input": "2024-02-14T21:07:56.183814Z",
          "iopub.status.idle": "2024-02-14T21:07:56.189884Z",
          "shell.execute_reply.started": "2024-02-14T21:07:56.183766Z",
          "shell.execute_reply": "2024-02-14T21:07:56.189116Z"
        },
        "trusted": true,
        "id": "POrcFnAJQW3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Metric (F1 Beta Score)"
      ],
      "metadata": {
        "id": "HBIpzjqUQW3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Big thanks to\n",
        "# https://www.kaggle.com/code/amedprof/pii-evaluation-metric?scriptVersionId=160040455\n",
        "def pii_fbeta_score(pred_df, gt_df,beta=5):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    - pred_df (DataFrame): DataFrame containing predicted PII labels.\n",
        "    - gt_df (DataFrame): DataFrame containing ground truth PII labels.\n",
        "    - beta (float): The beta parameter for the F-beta score, controlling the trade-off between precision and recall.\n",
        "\n",
        "    Returns:\n",
        "    - float: Micro F-beta score.\n",
        "    \"\"\"\n",
        "    df = pred_df.merge(gt_df,how='outer',on=['document',\"token\"],suffixes=('_pred','_gt'))\n",
        "\n",
        "    df['cm'] = \"\"\n",
        "\n",
        "    df.loc[df.label_gt.isna(),'cm'] = \"FP\"\n",
        "\n",
        "\n",
        "    df.loc[df.label_pred.isna(),'cm'] = \"FN\"\n",
        "    df.loc[(df.label_gt.notna()) & (df.label_gt!=df.label_pred),'cm'] = \"FN\"\n",
        "\n",
        "    df.loc[(df.label_pred.notna()) & (df.label_gt.notna()) & (df.label_gt==df.label_pred),'cm'] = \"TP\"\n",
        "\n",
        "    FP = (df['cm']==\"FP\").sum()\n",
        "    FN = (df['cm']==\"FN\").sum()\n",
        "    TP = (df['cm']==\"TP\").sum()\n",
        "\n",
        "    s_micro = (1+(beta**2))*TP/(((1+(beta**2))*TP) + ((beta**2)*FN) + FP)\n",
        "\n",
        "    return s_micro"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:07:56.192872Z",
          "iopub.execute_input": "2024-02-14T21:07:56.193441Z",
          "iopub.status.idle": "2024-02-14T21:07:56.326886Z",
          "shell.execute_reply.started": "2024-02-14T21:07:56.193402Z",
          "shell.execute_reply": "2024-02-14T21:07:56.325768Z"
        },
        "trusted": true,
        "id": "DcguDtAKQW3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Datasets"
      ],
      "metadata": {
        "id": "Y3S6J5_7QW3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Original Data"
      ],
      "metadata": {
        "id": "cNWTcJJKQW3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"))\n",
        "print(f\"len(train_df):{len(train_df)}, train_df[0].keys(): {list(train_df[0].keys())}\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "test_df = json.load(open('/kaggle/input/pii-detection-removal-from-educational-data/test.json'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:07:56.328305Z",
          "iopub.execute_input": "2024-02-14T21:07:56.328664Z",
          "iopub.status.idle": "2024-02-14T21:07:59.341266Z",
          "shell.execute_reply.started": "2024-02-14T21:07:56.328631Z",
          "shell.execute_reply": "2024-02-14T21:07:59.340145Z"
        },
        "trusted": true,
        "id": "e6hvP6f6QW3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load External Data (if needed)"
      ],
      "metadata": {
        "id": "2qcMUYcpQW3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG.use_external_train_data:\n",
        "    # Convert the \"stringified lists\" in the columns to proper Python lists\n",
        "    df_train_external = pd.read_csv('/kaggle/input/pii-external-dataset/pii_dataset.csv', converters={\n",
        "        'tokens': literal_eval,\n",
        "        'labels': literal_eval,\n",
        "        'trailing_whitespace': literal_eval\n",
        "    })\n",
        "    df_train_external.rename(columns={'text': 'full_text'}, inplace=True)\n",
        "    # convert to format similar to how we load in the original data\n",
        "    df_train_external = df_train_external.to_dict('records')\n",
        "    train_df.extend(df_train_external)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:07:59.34256Z",
          "iopub.execute_input": "2024-02-14T21:07:59.343097Z",
          "iopub.status.idle": "2024-02-14T21:08:09.109617Z",
          "shell.execute_reply.started": "2024-02-14T21:07:59.343064Z",
          "shell.execute_reply": "2024-02-14T21:08:09.108497Z"
        },
        "trusted": true,
        "id": "Ppl9LOCYQW3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Data (if needed)"
      ],
      "metadata": {
        "id": "sr8_cNb4QW3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG.samples_testing != None:\n",
        "    train_df = random.sample(train_df, CONFIG.samples_testing)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:08:09.110941Z",
          "iopub.execute_input": "2024-02-14T21:08:09.11126Z",
          "iopub.status.idle": "2024-02-14T21:08:09.116638Z",
          "shell.execute_reply.started": "2024-02-14T21:08:09.111231Z",
          "shell.execute_reply": "2024-02-14T21:08:09.115535Z"
        },
        "trusted": true,
        "id": "OSIKWKItQW3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_df length:\", len(train_df))\n",
        "\n",
        "labels = set()\n",
        "label_counts = {}\n",
        "for i in range(len(train_df)):\n",
        "    labels.update(train_df[i]['labels'])\n",
        "    for label in train_df[i]['labels']:\n",
        "        if label in label_counts:\n",
        "            label_counts[label] += 1\n",
        "        else:\n",
        "            label_counts[label] = 1\n",
        "\n",
        "print(f\"labels: {labels}\")\n",
        "print('-'*25)\n",
        "print(f\"label_counts: {label_counts}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:08:09.117932Z",
          "iopub.execute_input": "2024-02-14T21:08:09.118782Z",
          "iopub.status.idle": "2024-02-14T21:08:11.142065Z",
          "shell.execute_reply.started": "2024-02-14T21:08:09.11875Z",
          "shell.execute_reply": "2024-02-14T21:08:11.14089Z"
        },
        "trusted": true,
        "id": "kmTv4YpRQW3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Methods"
      ],
      "metadata": {
        "id": "4KpVLlk7QW3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_date(text):\n",
        "    try:\n",
        "        # Attempt to parse the text as a date\n",
        "        parsed_date = parser.parse(text)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def tokens2index(row):\n",
        "    tokens  = row['tokens']\n",
        "    start_ind = []\n",
        "    end_ind = []\n",
        "    prev_ind = 0\n",
        "    for tok in tokens:\n",
        "        start = prev_ind + row['full_text'][prev_ind:].index(tok)\n",
        "        end = start+len(tok)\n",
        "        start_ind.append(start)\n",
        "        end_ind.append(end)\n",
        "        prev_ind = end\n",
        "    return start_ind, end_ind\n",
        "\n",
        "# binary search\n",
        "def find_or_next_larger(arr, target):\n",
        "    left, right = 0, len(arr) - 1\n",
        "\n",
        "    while left <= right:\n",
        "        mid = (left + right) // 2\n",
        "\n",
        "        if arr[mid] == target:\n",
        "            return mid\n",
        "        elif arr[mid] < target:\n",
        "            left = mid + 1\n",
        "        else:\n",
        "            right = mid - 1\n",
        "    return left\n",
        "\n",
        "def count_trailing_whitespaces(word):\n",
        "    return len(word) - len(word.rstrip())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:08:11.144032Z",
          "iopub.execute_input": "2024-02-14T21:08:11.144887Z",
          "iopub.status.idle": "2024-02-14T21:08:11.156696Z",
          "shell.execute_reply.started": "2024-02-14T21:08:11.144843Z",
          "shell.execute_reply": "2024-02-14T21:08:11.155713Z"
        },
        "trusted": true,
        "id": "GJzIG7zEQW3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Analyzer\n",
        "\n",
        "For ease of code, we encapsulate the analyzer code in a class."
      ],
      "metadata": {
        "id": "pM7jOF8HQW3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyAnalyzer:\n",
        "\n",
        "    def __init__(self):\n",
        "        ## Initialize the analyzer\n",
        "        configuration = {\n",
        "            \"nlp_engine_name\": \"spacy\",\n",
        "            \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n",
        "        }\n",
        "\n",
        "        # Create NLP engine based on configuration\n",
        "        provider = NlpEngineProvider(nlp_configuration=configuration)\n",
        "        nlp_engine = provider.create_engine()\n",
        "\n",
        "        # create ID recognizer\n",
        "        id_regex = r'([A-Za-z]{2}[.?]:)?\\d{12,12}'\n",
        "        id_pattern = Pattern(name=\"id\", regex=id_regex, score = CONFIG.id_pattern_score)\n",
        "        id_recognizer = PatternRecognizer(supported_entity=\"ID_CUSTOM\", patterns = [id_pattern])\n",
        "\n",
        "        # create address recognizer\n",
        "        address_regex = r'\\b\\d+\\s+\\w+(\\s+\\w+)*\\s+((st(\\.)?)|(ave(\\.)?)|(rd(\\.)?)|(blvd(\\.)?)|(ln(\\.)?)|(ct(\\.)?)|(dr(\\.)?))\\b'\n",
        "        address_pattern = Pattern(name=\"address\", regex=address_regex, score = CONFIG.address_pattern_score)\n",
        "        address_recognizer = PatternRecognizer(supported_entity=\"ADDRESS_CUSTOM\", patterns = [address_pattern])\n",
        "\n",
        "        # create email recognizer\n",
        "        email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "        email_pattern = Pattern(name=\"email address\", regex=email_regex, score = CONFIG.email_pattern_score)\n",
        "        email_recognizer = PatternRecognizer(supported_entity=\"EMAIL_CUSTOM\", patterns = [email_pattern])\n",
        "\n",
        "        # create url recognizer\n",
        "        url_regex = r'https?://\\S+|www\\.\\S+'\n",
        "#         url_regex = r'https?:\\/\\/[a-zA-Z1-9.\\?\\=\\&]+'\n",
        "        url_pattern = Pattern(name=\"url\", regex=url_regex, score=CONFIG.url_pattern_score)\n",
        "        url_recognizer = PatternRecognizer(supported_entity=\"URL_CUSTOM\", patterns = [url_pattern])\n",
        "\n",
        "        registry = RecognizerRegistry()\n",
        "        registry.load_predefined_recognizers()\n",
        "        registry.add_recognizer(id_recognizer)\n",
        "        registry.add_recognizer(address_recognizer)\n",
        "        registry.add_recognizer(email_recognizer)\n",
        "        registry.add_recognizer(url_recognizer)\n",
        "\n",
        "        # Pass the created NLP engine and supported_languages to the AnalyzerEngine\n",
        "        self.analyzer = AnalyzerEngine(\n",
        "            nlp_engine=nlp_engine,\n",
        "            supported_languages=[\"en\"],\n",
        "            registry=registry\n",
        "        )\n",
        "\n",
        "        ## Initialize the black list\n",
        "        self.black_list = [\"wikipedia\", \"coursera\", \".pdf\", \".PDF\", \"article\",\n",
        "                           \".png\", \".gov\", \".work\", \".ai\", \".firm\", \".arts\",\n",
        "                           \".store\", \".rec\", \".biz\", \".travel\" ]\n",
        "\n",
        "\n",
        "    def predict_tokens(self, df_: list) -> pd.DataFrame:\n",
        "        \"\"\"Predict the tokens that have PII in the dataframe.\"\"\"\n",
        "\n",
        "        # list of all predictions for each label\n",
        "        PHONE_NUM, NAME_STUDENT, URL_PERSONAL, EMAIL, STREET_ADDRESS, ID_NUM, USERNAME = [],[],[],[],[],[], []\n",
        "\n",
        "        preds = []\n",
        "\n",
        "        # Find the starting and ending positions of each word after segmentation\n",
        "        for i in tqdm(range(len(df_)), desc=\"Processing tokens2index\"):\n",
        "            start, end = tokens2index(df_[i])\n",
        "            df_[i]['start'] = start\n",
        "            df_[i]['end'] = end\n",
        "\n",
        "        for i, d in tqdm(enumerate(df_), total=len(df_), desc=\"Analyzing entities\"):\n",
        "            #results:[type: PERSON, start: 22, end: 37, score: 0.85]\n",
        "            results = self.analyzer.analyze(text=d['full_text'],\n",
        "                                   entities=[\n",
        "                                             #\"PHONE_NUMBER\",\n",
        "                                             \"PERSON\",\n",
        "                                             \"URL_CUSTOM\", #\"IP_ADDRESS\", #\"URL\",\n",
        "                                             \"EMAIL_ADDRESS\", \"EMAIL_CUSTOM\",\n",
        "                                             \"ADDRESS_CUSTOM\",\n",
        "                                             \"US_SSN\", \"US_ITIN\", \"US_PASSPORT\", \"US_BANK_NUMBER\",\n",
        "                                             \"USERNAME\"],\n",
        "                                   language='en',\n",
        "        #                            score_threshold=0.2,\n",
        "                                    )\n",
        "            pre_preds = []\n",
        "\n",
        "            # Traverse each entity found,\n",
        "            # r: [type: PERSON, start: 22, end: 37, score: 0.85]\n",
        "            for r in results:\n",
        "                # That is, the sth word is the beginning of an entity\n",
        "                s = find_or_next_larger(d['start'], r.start)# d['start'][s] = r.start\n",
        "\n",
        "                end = r.end # entity end point\n",
        "                # find words in text\n",
        "                word = d['full_text'][r.start:r.end]\n",
        "                end = end - count_trailing_whitespaces(word)\n",
        "                temp_preds = [s]\n",
        "\n",
        "                try:\n",
        "                    while d['end'][s+1] <= end:\n",
        "                        temp_preds.append(s+1)\n",
        "                        s +=1\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                tmp = False\n",
        "\n",
        "                if r.entity_type == 'USERNAME':\n",
        "                    label = 'USERNAME'\n",
        "                    USERNAME.append(d['full_text'][r.start:r.end])\n",
        "\n",
        "        #         if r.entity_type == 'PHONE_NUMBER':\n",
        "        #             if is_valid_date(word):\n",
        "        #                 continue\n",
        "        #             label =  'PHONE_NUM'\n",
        "        #             PHONE_NUM.append(d['full_text'][r.start:r.end])\n",
        "\n",
        "                if r.entity_type == 'PERSON':\n",
        "                    label = 'NAME_STUDENT'\n",
        "                    NAME_STUDENT.append(d['full_text'][r.start:r.end])\n",
        "\n",
        "                if r.entity_type == 'ADDRESS_CUSTOM':\n",
        "                    label = 'STREET_ADDRESS'\n",
        "                    STREET_ADDRESS.append(d['full_text'][r.start:r.end])\n",
        "\n",
        "                if r.entity_type == 'ID_CUSTOM' or \\\n",
        "                    r.entity_type == 'US_SSN' or r.entity_type == 'US_ITIN' or r.entity_type == 'US_PASSPORT' or r.entity_type == 'US_BANK_NUMBER':\n",
        "\n",
        "                    label = 'ID_NUM'\n",
        "                    ID_NUM.append(d['full_text'][r.start:r.end])\n",
        "\n",
        "                if r.entity_type == 'EMAIL_ADDRESS' or r.entity_type == 'EMAIL_CUSTOM':\n",
        "                    label = 'EMAIL'\n",
        "                    EMAIL.append(d['full_text'][r.start:r.end])\n",
        "\n",
        "                if r.entity_type == 'URL_CUSTOM':# or r.entity_type == 'IP_ADDRESS' or \"http\" in word:\n",
        "                    for w in self.black_list:\n",
        "                        if w in word:\n",
        "                            tmp = True\n",
        "                            break\n",
        "\n",
        "                    label = 'URL_PERSONAL'\n",
        "                    URL_PERSONAL.append(d['full_text'][r.start:r.end])\n",
        "\n",
        "                if tmp:\n",
        "                    continue\n",
        "\n",
        "                for p in temp_preds:\n",
        "                    if len(pre_preds) > 0:\n",
        "                        \"\"\"\n",
        "                        When starting a new r, pre_preds[-1]['rlabel']\n",
        "                        is still the r.entity_type of the previous entity\n",
        "                        At this time, it may not be equal to the\n",
        "                        r.entity_type of this entity.\n",
        "\n",
        "                        In other words,\n",
        "                        the first equal sign is still in the same entity.\n",
        "                        \"\"\"\n",
        "                        if pre_preds[-1]['rlabel'] == r.entity_type and (p - pre_preds[-1]['token']==1):\n",
        "                            label_f = \"I-\"+label\n",
        "                        else:\n",
        "                            label_f = \"B-\"+label\n",
        "                    else:\n",
        "                        label_f = \"B-\"+label\n",
        "\n",
        "                    # pre_preds contains the output that we want\n",
        "                    pre_preds.append(({\n",
        "                        \"document\": d['document'],\n",
        "                        \"token\": p,\n",
        "                        \"label\": label_f,\n",
        "                        \"rlabel\": r.entity_type,\n",
        "                    }))\n",
        "\n",
        "            # After traversing this data, summarize all found entities\n",
        "            # and extend the preds for this document into the aggregate preds\n",
        "            preds.extend(pre_preds)\n",
        "\n",
        "        preds_df = pd.DataFrame(preds).iloc[:,:-1].reset_index()\n",
        "        return preds_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:08:11.158352Z",
          "iopub.execute_input": "2024-02-14T21:08:11.15911Z",
          "iopub.status.idle": "2024-02-14T21:08:11.191759Z",
          "shell.execute_reply.started": "2024-02-14T21:08:11.159063Z",
          "shell.execute_reply": "2024-02-14T21:08:11.190838Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "8zN8jJ-jQW3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Train Set"
      ],
      "metadata": {
        "id": "JV_fByPFQW3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = MyAnalyzer()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:08:11.193282Z",
          "iopub.execute_input": "2024-02-14T21:08:11.194009Z",
          "iopub.status.idle": "2024-02-14T21:08:17.329663Z",
          "shell.execute_reply.started": "2024-02-14T21:08:11.19395Z",
          "shell.execute_reply": "2024-02-14T21:08:17.328432Z"
        },
        "trusted": true,
        "id": "9fHzTr4DQW3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG.run_on_train_data:\n",
        "    train_preds = analyzer.predict_tokens(train_df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:08:17.333004Z",
          "iopub.execute_input": "2024-02-14T21:08:17.333552Z",
          "iopub.status.idle": "2024-02-14T21:30:34.97736Z",
          "shell.execute_reply.started": "2024-02-14T21:08:17.333514Z",
          "shell.execute_reply": "2024-02-14T21:30:34.976326Z"
        },
        "trusted": true,
        "id": "Vs6TVhQPQW3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Performance on Training Set"
      ],
      "metadata": {
        "id": "OaJwmVOKQW3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Corresponding DataFrame for \"True\" Answers"
      ],
      "metadata": {
        "id": "osE2yxhDQW3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG.run_on_train_data:\n",
        "    train_act_records = []\n",
        "    count = 0\n",
        "    for entry in train_df:\n",
        "        for idx, (token, label) in enumerate(zip(entry[\"tokens\"], entry[\"labels\"])):\n",
        "            if label != 'O':\n",
        "                train_act_records.append({\n",
        "                    'row_id': count,\n",
        "                    'document': entry[\"document\"],\n",
        "                    'token': idx,\n",
        "                    'label': label,\n",
        "                })\n",
        "                count += 1\n",
        "\n",
        "    train_act = pd.DataFrame.from_records(train_act_records)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:30:34.981457Z",
          "iopub.execute_input": "2024-02-14T21:30:34.981811Z",
          "iopub.status.idle": "2024-02-14T21:30:36.808264Z",
          "shell.execute_reply.started": "2024-02-14T21:30:34.98178Z",
          "shell.execute_reply": "2024-02-14T21:30:36.807164Z"
        },
        "trusted": true,
        "id": "GXiMWFNIQW3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that we haven't missed out any true values when making the \"train_act_records\"\n",
        "if CONFIG.run_on_train_data:\n",
        "    assert len(train_act) == sum(label_counts.values()) - label_counts['O'], \\\n",
        "        'mismatch between number of true labels in label_counts and train_act_records'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:30:36.810032Z",
          "iopub.execute_input": "2024-02-14T21:30:36.810404Z",
          "iopub.status.idle": "2024-02-14T21:30:36.816099Z",
          "shell.execute_reply.started": "2024-02-14T21:30:36.810371Z",
          "shell.execute_reply": "2024-02-14T21:30:36.814818Z"
        },
        "trusted": true,
        "id": "U__eKaKEQW3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pred_act_lists_for_dfs(preds: pd.DataFrame, act: pd.DataFrame):\n",
        "    document_idx_list = [(ex[\"document\"], len(ex[\"tokens\"])) for ex in train_df]\n",
        "\n",
        "    preds_list = []\n",
        "    act_list = []\n",
        "\n",
        "    for document, len_tokens in tqdm(document_idx_list, total=len(document_idx_list)):\n",
        "        preds_doc = preds[preds[\"document\"] == document].sort_values(by=\"token\")\n",
        "        act_doc = act[act[\"document\"] == document].sort_values(by=\"token\")\n",
        "\n",
        "        # We do a \"merge\" (like in mergesort) to combine the results from the preds and\n",
        "        # actual values\n",
        "        preds_idx = 0\n",
        "        act_idx = 0\n",
        "        preds_list_sub = []\n",
        "        act_list_sub = []\n",
        "        for i in range(len_tokens):\n",
        "            preds_head, act_head = None, None\n",
        "            if preds_idx < len(preds_doc):\n",
        "                preds_head = preds_doc.iloc[preds_idx]\n",
        "            if act_idx < len(act_doc):\n",
        "                act_head = act_doc.iloc[act_idx]\n",
        "\n",
        "            if act_head is not None and act_head[\"token\"] == i:\n",
        "                act_list_sub.append(act_head[\"label\"])\n",
        "                act_idx += 1\n",
        "            else:\n",
        "                act_list_sub.append('O')\n",
        "\n",
        "            if preds_head is not None and preds_head[\"token\"] == i:\n",
        "                preds_list_sub.append(preds_head[\"label\"])\n",
        "                preds_idx += 1\n",
        "            else:\n",
        "                preds_list_sub.append('O')\n",
        "\n",
        "        preds_list.extend(preds_list_sub)\n",
        "        act_list.extend(act_list_sub)\n",
        "\n",
        "    return preds_list, act_list\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:30:36.81807Z",
          "iopub.execute_input": "2024-02-14T21:30:36.818495Z",
          "iopub.status.idle": "2024-02-14T21:30:36.831531Z",
          "shell.execute_reply.started": "2024-02-14T21:30:36.818453Z",
          "shell.execute_reply": "2024-02-14T21:30:36.830509Z"
        },
        "trusted": true,
        "id": "VSSPAsuZQW3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG.run_on_train_data:\n",
        "    print(\"PII micro F-beta score:\", pii_fbeta_score(train_preds, train_act, beta = 5))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:30:36.832493Z",
          "iopub.execute_input": "2024-02-14T21:30:36.832826Z",
          "iopub.status.idle": "2024-02-14T21:30:36.998867Z",
          "shell.execute_reply.started": "2024-02-14T21:30:36.832796Z",
          "shell.execute_reply": "2024-02-14T21:30:36.997749Z"
        },
        "trusted": true,
        "id": "hlFFjHc3QW3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG.run_on_train_data:\n",
        "    train_preds_list, train_act_list = get_pred_act_lists_for_dfs(train_preds, train_act)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-14T21:30:37.000437Z",
          "iopub.execute_input": "2024-02-14T21:30:37.001156Z"
        },
        "trusted": true,
        "id": "Q4HYx5hVQW3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure that we processed the actual train list properly\n",
        "if CONFIG.run_on_train_data:\n",
        "    assert dict(Counter(train_act_list)) == label_counts, 'mismatch between label counts in label_counts and train_act_list'"
      ],
      "metadata": {
        "trusted": true,
        "id": "KQjPjP3xQW3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG.run_on_train_data:\n",
        "    print(\"-\"*25)\n",
        "    print(\"Counter for predicted labels:\")\n",
        "    print(\"-\"*25)\n",
        "    pprint.pprint(dict(Counter(train_preds_list)))\n",
        "    print()\n",
        "\n",
        "    print(\"-\"*25)\n",
        "    print(\"Counter for actual labels:\")\n",
        "    print(\"-\"*25)\n",
        "    pprint.pprint(dict(Counter(train_act_list)))\n",
        "    print()"
      ],
      "metadata": {
        "trusted": true,
        "id": "wdtDL5XfQW3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Report on Training Set"
      ],
      "metadata": {
        "id": "dPM83CktQW3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG.run_on_train_data:\n",
        "    print(classification_report(train_preds_list, train_act_list, digits=4))"
      ],
      "metadata": {
        "trusted": true,
        "id": "-6vOzHiPQW3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F Beta Score on Training Set"
      ],
      "metadata": {
        "id": "x5gA2M-bQW3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if CONFIG.run_on_train_data:\n",
        "#     print(\"Micro F1 Beta Score:\", score(train_preds_list, train_act_list))\n",
        "#     print(\"Macro F1 Beta Score:\", macro_score(train_preds_list, train_act_list))"
      ],
      "metadata": {
        "trusted": true,
        "id": "3UXmJcE3QW3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG.run_on_train_data:\n",
        "    del train_preds_list, train_act_list, train_preds, train_act\n",
        "    gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "QCb24PxhQW3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Test Set"
      ],
      "metadata": {
        "id": "M6WAKHDhQW3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = analyzer.predict_tokens(test_df)\n",
        "test_preds.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "pYtXaUkaQW3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission"
      ],
      "metadata": {
        "id": "N0ljMLd0QW3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame(test_preds)\n",
        "submission.columns = ['row_id','document', 'token', 'label']\n",
        "submission.to_csv('submission.csv', index = False)\n",
        "submission.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "XLpmuWgkQW3T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}